{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science Programming - Final Project\n",
    "### Assessing Post-Natural Disaster Building Damage via Satellite Imagery\n",
    "#### Team members: Alysson De Oliveira Silveira, Daniel Cordner, Manuel Salgueiro Gentile, Saahil Rasheed, Sai Sri Vathsava Udathu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model \n",
    "This notebook contains all the data preparation steps necessary to fit the Baseline Model.\n",
    "Note: Our models are presented in different notebooks and the data preparation part was reapeated in all of them, so they can work sepatelly. Anyway, if fitting more than one model in sequence, you may want to skip the data processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project aims to assess the damage caused by natural disasters using as input high quality satellite images.\n",
    "The dataset we used in this project was made available for the xView2 Challenge, and contains 2799 pairs of images, pre and post disaster.\n",
    "After downloaded and unzipped, the is split into two folders: image and labels. The first one, contains png pictures that can contains one or several buildings. \n",
    "The second folder contain json files that, among other information, displays the coordinates to each building and a label describing the level of demage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Preprocessing - Creating mask files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes shown below were adapted from 'https://github.com/DIUx-xView/xView2_baseline'.\n",
    "We modified the codes to simplify the project for several reasons:\n",
    "- lack of computational power\n",
    "- time constraints\n",
    "- this is the first experience with image analysis for all the members of our group.\n",
    "\n",
    "Ps. Just a few functions weren't adapated for this peoject. For those, included an observation to highlight that they are identical to the original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step was to create masks to delimit buildings, and to acomplish so, we had to get the coordinates from the json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path, walk, makedirs\n",
    "from sys import exit, stderr\n",
    "import os\n",
    "from cv2 import fillPoly\n",
    "import cv2\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "from shapely.geometry import mapping, Polygon\n",
    "from shapely.wkt import loads\n",
    "from skimage.io import imread\n",
    "from tqdm import tqdm\n",
    "import imantics \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\train'\n",
    "image_folder = path.join(root_folder, 'images')\n",
    "json_folder = path.join(root_folder, 'labels')\n",
    "mask_folder = path.join(root_folder, 'masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function below was copied from https://github.com/DIUx-xView/xView2_first_place/blob/master/create_masks.py without changes\n",
    "\n",
    "def mask_for_polygon(poly, im_size=(1024, 1024)):\n",
    "    img_mask = np.zeros(im_size, np.uint8)\n",
    "    int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "    exteriors = [int_coords(poly.exterior.coords)]\n",
    "    interiors = [int_coords(pi.coords) for pi in poly.interiors]\n",
    "    cv2.fillPoly(img_mask, exteriors, 1)\n",
    "    cv2.fillPoly(img_mask, interiors, 0)\n",
    "    return img_mask\n",
    "\n",
    "def process_image(json_file):\n",
    "    jfile = json.load(open(json_file))\n",
    "    msk = np.zeros((1024, 1024), dtype='uint8')\n",
    "    \n",
    "    for feat in jfile['features']['xy']:\n",
    "        poly = loads(feat['wkt'])\n",
    "        _msk = mask_for_polygon(poly)\n",
    "        msk[_msk > 0] = 255       \n",
    "    cv2.imwrite(json_file.replace('labels', 'masks').replace('.json', '.png'), msk, [cv2.IMWRITE_PNG_COMPRESSION, 9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting image dimensions\n",
    "def get_dims(chip_file):\n",
    "    pil_img = imread(chip_file)\n",
    "    img = np.array(pil_img)\n",
    "    w, h, c = img.shape\n",
    "    return w, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting polygon coordinates from json files       \n",
    "def get_coord(chip_json):\n",
    "    coordinates = {}\n",
    "    for i in chip_json['features']['xy']:\n",
    "        i_shape = wkt.loads(i['wkt'])\n",
    "        coords = list(mapping(i_shape)['coordinates'][0])\n",
    "        coordinates[i['properties']['uid']] = (np.array(coords, np.int32))\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mask(masked_polygons, mask_folder, mask_file_name):\n",
    "    for m in masked_polygons:\n",
    "        output = path.join(mask_folder, mask_file_name + '_{}.png'.format(m))\n",
    "        cv2.imwrite(output, masked_polygons[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsons(json_full_path):\n",
    "    jdata = json.load(open(json_full_path))\n",
    "    return jdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating through the folder containing json files and storing their names \n",
    "jsons = [j for j in next(walk(json_folder))[2]]\n",
    "\n",
    "#iterating through json files, and making sure that are no files with other formats among them\n",
    "for j in tqdm([j for j in jsons if j.endswith('json')], unit='poly', leave=False):\n",
    "        #split the name of the file and the format, and replace .json by .png to access the images, as they have the same name standard \n",
    "        chip_img_id = path.splitext(j)[0] + '.png'\n",
    "        #get file name without the extension. This will be used as the name of the mask file\n",
    "        mask_file_name = path.splitext(j)[0]\n",
    "        \n",
    "        #reading json files\n",
    "        json_full_path = path.join(json_folder, j)\n",
    "        chip_json = load_jsons(json_full_path)\n",
    "\n",
    "        #getting image dimensions\n",
    "        chip_file = path.join(image_folder, chip_img_id)\n",
    "        chip_size = get_dims(chip_file)\n",
    "\n",
    "        # getting polygon coordinates from json files\n",
    "        coordinates = get_coord(chip_json)\n",
    "        \n",
    "        #creating masks for each polygon using border=2 and saving mask images \n",
    "        process_image(json_full_path)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./train/images/hurricane-florence_00000001_pre_disaster.png', width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='./train/masks/hurricane-florence_00000001_pre_disaster.png', width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Preprocessing - Spliting images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most challenging aspects of this project is that the analysis is at the building level and the images are not. So, after we recognized the buildings in each image, using its coordinates, we now need to create a separate image for each building and link it with the respective label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing a new set of libraries to continue with the proprocessing\n",
    "from PIL import Image\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import shapely\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the labels are displayed in text format, we need to encode it so later we can add it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage = defaultdict(lambda: 0)\n",
    "damage['destroyed'] = 3\n",
    "damage['major-damage'] = 2\n",
    "damage['minor-damage'] = 1\n",
    "damage['no-damage'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as we are not using the folder structure suggested by Ritwik Gupta in https://github.com/DIUx-xView/xView2_baseline, the code is way simpler than the original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output'\n",
    "output_mask_folder = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the corners of each polygon.\n",
    "# As it is a polygon, we are using the max coordinate on both x and y axis to create a square image\n",
    "# The function was adapated to crop mask images as well\n",
    "\n",
    "def process_img(img_array, polygon_pts, scale_pct):\n",
    "    \"\"\"Process Raw Data into\n",
    "            Args:\n",
    "                img_array (numpy array): numpy representation of image.\n",
    "                polygon_pts (array): corners of the building polygon.\n",
    "            Returns:\n",
    "                numpy array: .\n",
    "    \"\"\"\n",
    "    if len(img_array.shape) == 3:\n",
    "        height, width, _ = img_array.shape\n",
    "    else:\n",
    "        height, width = img_array.shape\n",
    "\n",
    "    xcoords = polygon_pts[:, 0]\n",
    "    ycoords = polygon_pts[:, 1]\n",
    "    xmin, xmax = np.min(xcoords), np.max(xcoords)\n",
    "    ymin, ymax = np.min(ycoords), np.max(ycoords)\n",
    "\n",
    "    xdiff = xmax - xmin\n",
    "    ydiff = ymax - ymin\n",
    "\n",
    "    #Extend image by scale percentage\n",
    "    xmin = max(int(xmin - (xdiff * scale_pct)), 0)\n",
    "    xmax = min(int(xmax + (xdiff * scale_pct)), width)\n",
    "    ymin = max(int(ymin - (ydiff * scale_pct)), 0)\n",
    "    ymax = min(int(ymax + (ydiff * scale_pct)), height)\n",
    "\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        return img_array[ymin:ymax, xmin:xmax, :]\n",
    "    else:\n",
    "        return img_array[ymin:ymax, xmin:xmax]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating one separate image per polygon \n",
    "# We also calculated the image mean and subtracted from the original array\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "images = [i for i in next(walk(image_folder))[2]]\n",
    "for img in tqdm(images):\n",
    "    #Opening each image and transforming it into a numpy array and subtracting the image mean\n",
    "    imgage_obj = Image.open(path.join(image_folder, img))\n",
    "    image_array = np.array(imgage_obj)\n",
    "    #image_array = image_array - image_array.mean()\n",
    "        \n",
    "    #Getting the respective label for each picture\n",
    "    label_path = path.join(image_folder.replace('images', 'labels'), img.replace('png', 'json'))\n",
    "    label_file = open(label_path)\n",
    "    label_data = json.load(label_file)\n",
    "    \n",
    "    mask_path = path.join(image_folder.replace('images', 'masks'), img)\n",
    "    mask_obj = Image.open(mask_path)\n",
    "    mask_array = np.array(mask_obj)\n",
    "\n",
    "\n",
    "    for building in label_data['features']['xy']:\n",
    "        #As pre-disaster images don't have a label, we assinged no-demage to them\n",
    "        try:\n",
    "            damage_type = building['properties']['subtype']\n",
    "        except:\n",
    "            damage_type = \"no-damage\"\n",
    "            continue\n",
    "\n",
    "        #Creating two vectors, one with the label (y) and the other with the building uuid(X).\n",
    "        #Together whey will become a dataset thet will help us when training the model with flow_from_dataset\n",
    "        building_uuid = building['properties']['uid'] + \".png\"\n",
    "        y.append(damage[damage_type])\n",
    "        x.append(building_uuid)\n",
    "        \n",
    "        #Creating separate images for each building based on the corners of the polygons\n",
    "        #Same process is repeated for masks\n",
    "        i_shape = shapely.wkt.loads(building['wkt'])\n",
    "        polygon_corners = np.array(list(i_shape.exterior.coords))\n",
    "        poly_img = process_img(image_array, polygon_corners, 0.8)\n",
    "        poly_mask = process_img(mask_array, polygon_corners, 0.8)\n",
    "        cv2.imwrite(output_folder + \"/\" + building_uuid, poly_img)\n",
    "        cv2.imwrite(output_mask_folder + \"/\" + building_uuid, poly_mask)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating separate images for each buiding and also two vectors containig the labels and the buildings IDs, we used train_test_split() to create two datasets, one containing the observations that will be used for training, and the other containing the ones that will serve for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output CSV'\n",
    "train_csv = os.path.join(csv_path, \"train.csv\")\n",
    "test_csv = os.path.join(csv_path, \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "train = {'uuid': x_train, 'labels': y_train}\n",
    "test = {'uuid': x_test, 'labels': y_test}\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "train_df.to_csv(train_csv)\n",
    "\n",
    "test_df = pd.DataFrame(test)\n",
    "test_df.to_csv(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Fitting the model (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing new set of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shapely.wkt\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import ast\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Add, Input, Concatenate, Dropout\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We inserted this code to avoid receiving error massages while fitting the model after several epochs\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the GPU allocation to avoid running out of resources\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if GPU is available\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This model was adapted from https://github.com/DIUx-xView/xView2_baseline/blob/3b8ed914463ad0701a178521fd89e9582501fd92/model/model.py\n",
    "### We reduced the number of dense layers so the model could run in our notebooks\n",
    "### Originally it has:\n",
    "  #concated_layers = Dense(2024, activation='relu')(concated_layers)\n",
    "  #concated_layers = Dense(524, activation='relu')(concated_layers)\n",
    "  #concated_layers = Dense(124, activation='relu')(concated_layers)\n",
    "\n",
    "###\n",
    "# Loss function for ordinal loss from https://github.com/JHart96/keras_ordinal_categorical_crossentropy\n",
    "###\n",
    "def ordinal_loss(y_true, y_pred):\n",
    "    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n",
    "    return (1.0 + weights) * keras.losses.categorical_crossentropy(y_true, y_pred )\n",
    "\n",
    "\n",
    "###\n",
    "# Generate a simple CNN\n",
    "###\n",
    "def generate_xBD_baseline_model():\n",
    "  weights = 'imagenet'\n",
    "  inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "  base_model = ResNet50(include_top=False, weights=weights, input_shape=(128, 128, 3))\n",
    "\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "  x = Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 3))(inputs)\n",
    "  x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "  x = Dropout(rate=0.3)(x)\n",
    "\n",
    "  x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "  x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "  x = Dropout(rate=0.3)(x)\n",
    "\n",
    "  x = Flatten()(x)\n",
    "\n",
    "  base_resnet = base_model(inputs)\n",
    "  base_resnet = Flatten()(base_resnet)\n",
    "\n",
    "  concated_layers = Concatenate()([x, base_resnet])\n",
    "\n",
    "  concated_layers = Dense(524, activation='relu')(concated_layers)\n",
    "  concated_layers = Dense(254, activation='relu')(concated_layers)\n",
    "  concated_layers = Dense(124, activation='relu')(concated_layers)\n",
    "  concated_layers = Dropout(rate=0.5)(concated_layers)\n",
    "\n",
    "  output = Dense(4, activation='relu')(concated_layers)\n",
    "\n",
    "  model = Model(inputs=inputs, outputs=output)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reduced the batches size to decrease the required computational power\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Configurations\n",
    "NUM_WORKERS = 0 \n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01 #0.0001\n",
    "RANDOM_SEED = 123\n",
    "LOG_STEP = 150\n",
    "LOG_DIR = '/path/to/logs' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_intensity_encoding = dict()\n",
    "damage_intensity_encoding[3] = '3'\n",
    "damage_intensity_encoding[2] = '2' \n",
    "damage_intensity_encoding[1] = '1' \n",
    "damage_intensity_encoding[0] = '0' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function was not adapted and can be found in  https://github.com/DIUx-xView/xView2_baseline/blob/3b8ed914463ad0701a178521fd89e9582501fd92/model/model.py\n",
    "\n",
    "###\n",
    "# Function to compute unweighted f1 scores, just for reference\n",
    "###\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function was not adapted and can be found in  https://github.com/DIUx-xView/xView2_baseline/blob/3b8ed914463ad0701a178521fd89e9582501fd92/model/model.py\n",
    "###\n",
    "# Creates data generator for validation set\n",
    "###\n",
    "def validation_generator(test_csv, test_dir):\n",
    "    df = pd.read_csv(test_csv)\n",
    "    df = df.replace({\"labels\" : damage_intensity_encoding })\n",
    "\n",
    "    gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "                             rescale=1/255.)\n",
    "    return gen.flow_from_dataframe(dataframe=df,\n",
    "                                   directory=test_dir,\n",
    "                                   x_col='uuid',\n",
    "                                   y_col='labels',\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   shuffle=False,\n",
    "                                   seed=RANDOM_SEED,\n",
    "                                   class_mode=\"categorical\",\n",
    "                                   target_size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function was adapted to import masks together with images\n",
    "\n",
    "###\n",
    "# Applies random transformations to training data\n",
    "###\n",
    "def augment_data(df, train_data, mask_data):\n",
    "\n",
    "    df = df.replace({\"labels\" : damage_intensity_encoding })\n",
    "    arguments = dict(horizontal_flip=True,\n",
    "                     vertical_flip=True,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     zca_whitening=True,\n",
    "                     shear_range=0.1, \n",
    "                     zoom_range=0.1,\n",
    "                     channel_shift_range=0.1,\n",
    "                     rescale=1/255.)\n",
    "    \n",
    "    image_datagen = keras.preprocessing.image.ImageDataGenerator(**arguments)\n",
    "    #mask_datagen = keras.preprocessing.image.ImageDataGenerator(**arguments)\n",
    "    \n",
    "    image_generator =  image_datagen.flow_from_dataframe(dataframe=df,\n",
    "                                       directory=train_data,\n",
    "                                       x_col='uuid',\n",
    "                                       y_col='labels',\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       seed=RANDOM_SEED,\n",
    "                                       class_mode=\"categorical\",\n",
    "                                       target_size=(128, 128))\n",
    "\n",
    "    #mask_generator =  image_datagen.flow_from_dataframe(dataframe=df,\n",
    "    #                                   directory=mask_data,\n",
    "    #                                   x_col='uuid',\n",
    "    #                                   y_col='labels',\n",
    "    #                                   batch_size=BATCH_SIZE,\n",
    "    #                                   seed=RANDOM_SEED,\n",
    "    #                                   class_mode=None,\n",
    "    #                                   target_size=(128, 128))\n",
    "\n",
    "    #while True:\n",
    "    #    x_1 = image_generator.next()\n",
    "    #    x_2 = mask_generator.next()\n",
    "\n",
    "    #    yield [x_1[0], x_2[0]], x_1[1]\n",
    "    \n",
    "    return image_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function was adapted to import masks together with images\n",
    "\n",
    "def train_model(train_data, mask_data, train_csv, test_data, test_csv, model_in, model_out):\n",
    "\n",
    "    model = generate_xBD_baseline_model()\n",
    "\n",
    "    # Add model weights if provided by user\n",
    "    if model_in is not None:\n",
    "        model.load_weights(model_in)\n",
    "\n",
    "    df = pd.read_csv(train_csv)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(df['labels'].to_list()), df['labels'].to_list());\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    samples = df['uuid'].count()\n",
    "    steps = np.ceil((samples/BATCH_SIZE)-2)\n",
    "\n",
    "    # Augments the training data\n",
    "    train_gen_flow = augment_data(df, train_data, mask_data)\n",
    "    \n",
    "    #Set up tensorboard logging\n",
    "    tensorboard_callbacks = keras.callbacks.TensorBoard(log_dir=LOG_DIR)\n",
    "\n",
    "    \n",
    "    #Filepath to save model weights\n",
    "    filepath = model_out + \"\\simple-saved-model-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
    "    checkpoints = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                    monitor=['loss', 'accuracy'],\n",
    "                                                    verbose=1,\n",
    "                                                    save_best_only=False,\n",
    "                                                    mode='max')\n",
    "\n",
    "    #Adds adam optimizer\n",
    "    RMSprop = keras.optimizers.RMSprop(lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "    model.compile(loss=ordinal_loss, optimizer=RMSprop, metrics=['accuracy', f1])\n",
    "    \n",
    "    #Training begins\n",
    "    history = model.fit_generator(generator=train_gen_flow,\n",
    "                        steps_per_epoch=steps,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        workers=NUM_WORKERS,\n",
    "                        use_multiprocessing=True,\n",
    "                        class_weight=d_class_weights,\n",
    "                        callbacks=[tensorboard_callbacks, checkpoints],\n",
    "                        verbose=1)\n",
    "\n",
    "\n",
    "    #Evalulate f1 weighted scores on validation set\n",
    "    validation_gen = validation_generator(test_csv, test_data)\n",
    "    predictions = model.predict(validation_gen)\n",
    "\n",
    "    val_trues = validation_gen.classes\n",
    "    val_pred = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    f1_weighted = f1_score(val_trues, val_pred, average='weighted')\n",
    "    print(f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output'\n",
    "mask_data = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output Masks'\n",
    "train_csv = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output CSV\\train.csv'\n",
    "test_data = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output'\n",
    "test_csv = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output CSV\\test.csv'\n",
    "model_out = r'C:\\Users\\saahi\\Documents\\Assessing Building Damage\\Model Outputs\\Output CSV\\Model Output'\n",
    "model_in = None\n",
    "#model_in = './Model_out/saved-model-10-0.74.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130229 validated image filenames belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "2033/2033 [==============================] - 1269s 624ms/step - loss: 3.2368 - accuracy: 0.5276 - f1: 0.4537\n",
      "\n",
      "Epoch 00001: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-01-0.53.hdf5\n",
      "Epoch 2/10\n",
      "2033/2033 [==============================] - 1204s 592ms/step - loss: 2.1329 - accuracy: 0.6118 - f1: 0.4647\n",
      "\n",
      "Epoch 00002: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-02-0.61.hdf5\n",
      "Epoch 3/10\n",
      "2033/2033 [==============================] - 1209s 595ms/step - loss: 2.0885 - accuracy: 0.6305 - f1: 0.4736\n",
      "\n",
      "Epoch 00003: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-03-0.63.hdf5\n",
      "Epoch 4/10\n",
      "2033/2033 [==============================] - 1205s 593ms/step - loss: 2.1774 - accuracy: 0.6535 - f1: 0.4822\n",
      "\n",
      "Epoch 00004: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-04-0.65.hdf5\n",
      "Epoch 5/10\n",
      "2033/2033 [==============================] - 1208s 594ms/step - loss: 2.1290 - accuracy: 0.6579 - f1: 0.4789\n",
      "\n",
      "Epoch 00005: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-05-0.66.hdf5\n",
      "Epoch 6/10\n",
      "2033/2033 [==============================] - 1194s 587ms/step - loss: 2.0216 - accuracy: 0.6669 - f1: 0.4721\n",
      "\n",
      "Epoch 00006: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-06-0.67.hdf5\n",
      "Epoch 7/10\n",
      "2033/2033 [==============================] - 1194s 587ms/step - loss: 2.1876 - accuracy: 0.6674 - f1: 0.4867\n",
      "\n",
      "Epoch 00007: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-07-0.67.hdf5\n",
      "Epoch 8/10\n",
      "2033/2033 [==============================] - 1195s 588ms/step - loss: 2.2431 - accuracy: 0.6736 - f1: 0.4809\n",
      "\n",
      "Epoch 00008: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-08-0.67.hdf5\n",
      "Epoch 9/10\n",
      "2033/2033 [==============================] - 1194s 587ms/step - loss: 2.1924 - accuracy: 0.6574 - f1: 0.4972\n",
      "\n",
      "Epoch 00009: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-09-0.66.hdf5\n",
      "Epoch 10/10\n",
      "2033/2033 [==============================] - 1194s 587ms/step - loss: 2.2116 - accuracy: 0.6528 - f1: 0.4939\n",
      "\n",
      "Epoch 00010: saving model to C:\\Users\\alyss\\Desktop\\MS BAIS\\Fall 2020\\ISM6251 - Data Science Programming\\Final Project\\Model_out\\simple-saved-model-10-0.65.hdf5\n",
      "Found 32558 validated image filenames belonging to 4 classes.\n",
      "0.012128147188808013\n"
     ]
    }
   ],
   "source": [
    "train_model(train_data, mask_data, train_csv, test_data, test_csv, model_in, model_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
